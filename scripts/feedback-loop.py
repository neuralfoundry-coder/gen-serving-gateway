#!/usr/bin/env python3
"""
Feedback Loop Implementation
Automatically analyzes test results, generates improvements documentation,
and creates actionable tasks based on performance metrics.
"""

import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse


class FeedbackLoop:
    """Implements continuous improvement feedback loop"""
    
    def __init__(self, project_dir: str):
        self.project_dir = Path(project_dir)
        self.reports_dir = self.project_dir / "reports"
        self.latest_dir = self.reports_dir / "latest"
        self.history_dir = self.reports_dir / "history"
        
        # Performance thresholds
        self.thresholds = {
            "p95_duration_ms": 500,
            "p99_duration_ms": 1000,
            "error_rate": 0.02,  # 2%
            "min_throughput": 50,  # req/s
        }
        
        # Improvement categories
        self.categories = {
            "critical": [],
            "high": [],
            "medium": [],
            "low": [],
        }
    
    def load_latest_results(self) -> Dict[str, Dict]:
        """Load all latest test results"""
        results = {}
        scenarios = ["baseline", "spike", "stress", "soak", "breakpoint"]
        
        for scenario in scenarios:
            result_file = self.latest_dir / f"{scenario}.json"
            if result_file.exists():
                with open(result_file) as f:
                    results[scenario] = json.load(f)
        
        return results
    
    def analyze_results(self, results: Dict[str, Dict]) -> Dict:
        """Analyze results and identify issues"""
        issues = []
        improvements = []
        metrics_summary = {}
        
        for scenario, data in results.items():
            summary = data.get("summary", {})
            analysis = data.get("analysis", {})
            
            metrics_summary[scenario] = {
                "avg_duration_ms": summary.get("avg_duration_ms", 0),
                "p95_duration_ms": summary.get("p95_duration_ms", 0),
                "p99_duration_ms": summary.get("p99_duration_ms", 0),
                "error_rate": summary.get("error_rate", 0),
                "throughput": summary.get("requests_per_second", 0),
            }
            
            # Check against thresholds
            p95 = summary.get("p95_duration_ms", 0)
            p99 = summary.get("p99_duration_ms", 0)
            error_rate = summary.get("error_rate", 0)
            throughput = summary.get("requests_per_second", 0)
            
            # P95 latency check
            if p95 > self.thresholds["p95_duration_ms"]:
                severity = "critical" if p95 > self.thresholds["p95_duration_ms"] * 2 else "high"
                issues.append({
                    "severity": severity,
                    "scenario": scenario,
                    "type": "latency",
                    "metric": "p95_duration_ms",
                    "value": p95,
                    "threshold": self.thresholds["p95_duration_ms"],
                    "description": f"P95 ÏùëÎãµ ÏãúÍ∞ÑÏù¥ ÏûÑÍ≥ÑÍ∞í Ï¥àÍ≥º ({p95:.0f}ms > {self.thresholds['p95_duration_ms']}ms)",
                })
            
            # P99 latency check
            if p99 > self.thresholds["p99_duration_ms"]:
                severity = "critical" if p99 > self.thresholds["p99_duration_ms"] * 2 else "high"
                issues.append({
                    "severity": severity,
                    "scenario": scenario,
                    "type": "latency",
                    "metric": "p99_duration_ms",
                    "value": p99,
                    "threshold": self.thresholds["p99_duration_ms"],
                    "description": f"P99 ÏùëÎãµ ÏãúÍ∞ÑÏù¥ ÏûÑÍ≥ÑÍ∞í Ï¥àÍ≥º ({p99:.0f}ms > {self.thresholds['p99_duration_ms']}ms)",
                })
            
            # Error rate check
            if error_rate > self.thresholds["error_rate"]:
                severity = "critical" if error_rate > 0.1 else "high"
                issues.append({
                    "severity": severity,
                    "scenario": scenario,
                    "type": "reliability",
                    "metric": "error_rate",
                    "value": error_rate,
                    "threshold": self.thresholds["error_rate"],
                    "description": f"ÏóêÎü¨Ïú®Ïù¥ ÏûÑÍ≥ÑÍ∞í Ï¥àÍ≥º ({error_rate*100:.2f}% > {self.thresholds['error_rate']*100}%)",
                })
            
            # Throughput check
            if throughput < self.thresholds["min_throughput"] and throughput > 0:
                issues.append({
                    "severity": "medium",
                    "scenario": scenario,
                    "type": "performance",
                    "metric": "throughput",
                    "value": throughput,
                    "threshold": self.thresholds["min_throughput"],
                    "description": f"Ï≤òÎ¶¨ÎüâÏù¥ Í∏∞Ï§Ä ÎØ∏Îã¨ ({throughput:.1f} < {self.thresholds['min_throughput']} req/s)",
                })
            
            # Add scenario-specific recommendations
            for rec in analysis.get("recommendations", []):
                improvements.append({
                    "scenario": scenario,
                    "recommendation": rec,
                })
        
        return {
            "timestamp": datetime.now().isoformat(),
            "metrics_summary": metrics_summary,
            "issues": issues,
            "improvements": improvements,
            "issue_count": {
                "critical": len([i for i in issues if i["severity"] == "critical"]),
                "high": len([i for i in issues if i["severity"] == "high"]),
                "medium": len([i for i in issues if i["severity"] == "medium"]),
                "low": len([i for i in issues if i["severity"] == "low"]),
            }
        }
    
    def generate_action_items(self, analysis: Dict) -> List[Dict]:
        """Generate actionable improvement items"""
        action_items = []
        
        issues = analysis.get("issues", [])
        
        # Group issues by type
        latency_issues = [i for i in issues if i["type"] == "latency"]
        reliability_issues = [i for i in issues if i["type"] == "reliability"]
        performance_issues = [i for i in issues if i["type"] == "performance"]
        
        # Generate action items for latency issues
        if latency_issues:
            action_items.append({
                "id": "ACTION-001",
                "title": "ÏùëÎãµ ÏãúÍ∞Ñ ÏµúÏ†ÅÌôî",
                "priority": "high" if any(i["severity"] == "critical" for i in latency_issues) else "medium",
                "description": "ÎÜíÏùÄ ÏùëÎãµ ÏãúÍ∞ÑÏù¥ Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§.",
                "tasks": [
                    "Î∞±ÏóîÎìú ÏÑúÎ≤Ñ ÏùëÎãµ ÏãúÍ∞Ñ ÌîÑÎ°úÌååÏùºÎßÅ",
                    "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏøºÎ¶¨ ÏµúÏ†ÅÌôî Í≤ÄÌÜ†",
                    "Ï∫êÏã± Ï†ÑÎûµ Íµ¨ÌòÑ ÎòêÎäî Í∞úÏÑ†",
                    "Ïó∞Í≤∞ ÌíÄÎßÅ ÏÑ§Ï†ï Í≤ÄÌÜ†",
                ],
                "affected_scenarios": list(set(i["scenario"] for i in latency_issues)),
                "metrics": {
                    "current_p95": max(i["value"] for i in latency_issues),
                    "target_p95": self.thresholds["p95_duration_ms"],
                }
            })
        
        # Generate action items for reliability issues
        if reliability_issues:
            action_items.append({
                "id": "ACTION-002",
                "title": "Ïã†Î¢∞ÏÑ± Í∞úÏÑ†",
                "priority": "critical" if any(i["severity"] == "critical" for i in reliability_issues) else "high",
                "description": "ÎÜíÏùÄ ÏóêÎü¨Ïú®Ïù¥ Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§.",
                "tasks": [
                    "ÏóêÎü¨ Î°úÍ∑∏ Î∂ÑÏÑù Î∞è Í∑ºÎ≥∏ ÏõêÏù∏ ÌååÏïÖ",
                    "Î∞±ÏóîÎìú Ìó¨Ïä§Ï≤¥ÌÅ¨ Í∞ïÌôî",
                    "Ïû¨ÏãúÎèÑ Î°úÏßÅ Î∞è ÏÑúÌÇ∑ Î∏åÎ†àÏù¥Ïª§ Í≤ÄÌÜ†",
                    "ÌÉÄÏûÑÏïÑÏõÉ ÏÑ§Ï†ï ÏµúÏ†ÅÌôî",
                ],
                "affected_scenarios": list(set(i["scenario"] for i in reliability_issues)),
                "metrics": {
                    "current_error_rate": max(i["value"] for i in reliability_issues),
                    "target_error_rate": self.thresholds["error_rate"],
                }
            })
        
        # Generate action items for performance issues
        if performance_issues:
            action_items.append({
                "id": "ACTION-003",
                "title": "Ï≤òÎ¶¨Îüâ Í∞úÏÑ†",
                "priority": "medium",
                "description": "Ï≤òÎ¶¨ÎüâÏù¥ Í∏∞Ï§ÄÏóê ÎØ∏Îã¨Ìï©ÎãàÎã§.",
                "tasks": [
                    "ÎèôÏãúÏÑ± ÏÑ§Ï†ï Í≤ÄÌÜ† Î∞è ÏµúÏ†ÅÌôî",
                    "Î¶¨ÏÜåÏä§ Ï†úÌïú ÌôïÏù∏ (CPU, Î©îÎ™®Î¶¨)",
                    "Î°úÎìú Î∞∏Îü∞Ïã± Ï†ÑÎûµ Í≤ÄÌÜ†",
                    "ÏàòÌèâÏ†Å ÌôïÏû• Í≥†Î†§",
                ],
                "affected_scenarios": list(set(i["scenario"] for i in performance_issues)),
                "metrics": {
                    "current_throughput": min(i["value"] for i in performance_issues),
                    "target_throughput": self.thresholds["min_throughput"],
                }
            })
        
        return action_items
    
    def generate_improvements_document(self, analysis: Dict, action_items: List[Dict]) -> str:
        """Generate markdown document with improvements"""
        
        doc = f"""# ÌÖåÏä§Ìä∏ Í≤∞Í≥º Î∂ÑÏÑù Î∞è Í∞úÏÑ† Í≥ÑÌöç

ÏÉùÏÑ± ÏãúÍ∞Ñ: {analysis['timestamp']}

## 1. ÏöîÏïΩ

### Î∞úÍ≤¨Îêú Ïù¥Ïäà
- üî¥ Critical: {analysis['issue_count']['critical']}Í±¥
- üü† High: {analysis['issue_count']['high']}Í±¥
- üü° Medium: {analysis['issue_count']['medium']}Í±¥
- üü¢ Low: {analysis['issue_count']['low']}Í±¥

### ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏöîÏïΩ

| ÏãúÎÇòÎ¶¨Ïò§ | ÌèâÍ∑† ÏùëÎãµ | P95 ÏùëÎãµ | P99 ÏùëÎãµ | ÏóêÎü¨Ïú® | Ï≤òÎ¶¨Îüâ |
|---------|----------|---------|---------|-------|--------|
"""
        
        for scenario, metrics in analysis['metrics_summary'].items():
            doc += f"| {scenario} | {metrics['avg_duration_ms']:.0f}ms | {metrics['p95_duration_ms']:.0f}ms | {metrics['p99_duration_ms']:.0f}ms | {metrics['error_rate']*100:.2f}% | {metrics['throughput']:.1f}/s |\n"
        
        doc += """
## 2. Î∞úÍ≤¨Îêú Ïù¥Ïäà ÏÉÅÏÑ∏

"""
        
        for issue in analysis['issues']:
            severity_icon = {
                "critical": "üî¥",
                "high": "üü†",
                "medium": "üü°",
                "low": "üü¢"
            }.get(issue['severity'], "‚ö™")
            
            doc += f"""### {severity_icon} [{issue['severity'].upper()}] {issue['description']}

- **ÏãúÎÇòÎ¶¨Ïò§**: {issue['scenario']}
- **Î©îÌä∏Î¶≠**: {issue['metric']}
- **ÌòÑÏû¨ Í∞í**: {issue['value']:.2f}
- **ÏûÑÍ≥ÑÍ∞í**: {issue['threshold']}

"""
        
        doc += """## 3. Ï°∞Ïπò Í≥ÑÌöç

"""
        
        for item in action_items:
            priority_icon = {
                "critical": "üî¥",
                "high": "üü†",
                "medium": "üü°",
                "low": "üü¢"
            }.get(item['priority'], "‚ö™")
            
            doc += f"""### {priority_icon} {item['id']}: {item['title']}

**Ïö∞ÏÑ†ÏàúÏúÑ**: {item['priority'].upper()}
**ÏÑ§Î™Ö**: {item['description']}
**ÏòÅÌñ• ÏãúÎÇòÎ¶¨Ïò§**: {', '.join(item['affected_scenarios'])}

**ÏÑ∏Î∂Ä ÏûëÏóÖ**:
"""
            for task in item['tasks']:
                doc += f"- [ ] {task}\n"
            
            doc += f"""
**Î™©Ìëú Î©îÌä∏Î¶≠**:
"""
            for metric, value in item['metrics'].items():
                if isinstance(value, float):
                    doc += f"- {metric}: {value:.2f}\n"
                else:
                    doc += f"- {metric}: {value}\n"
            
            doc += "\n"
        
        doc += """## 4. Îã§Ïùå Îã®Í≥Ñ

1. ÏúÑ Ï°∞Ïπò Í≥ÑÌöçÏùÑ Í≤ÄÌÜ†ÌïòÍ≥† Ïö∞ÏÑ†ÏàúÏúÑÎ•º ÌôïÏ†ïÌï©ÎãàÎã§.
2. Critical/High Ïö∞ÏÑ†ÏàúÏúÑ Ìï≠Î™©Î∂ÄÌÑ∞ ÏûëÏóÖÏùÑ ÏãúÏûëÌï©ÎãàÎã§.
3. Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ Ï†ÅÏö© ÌõÑ ÎèôÏùºÌïú ÌÖåÏä§Ìä∏Î•º Ïû¨Ïã§ÌñâÌï©ÎãàÎã§.
4. Í∞úÏÑ† Ïó¨Î∂ÄÎ•º ÌôïÏù∏ÌïòÍ≥† Î¨∏ÏÑúÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.

## 5. ÌûàÏä§ÌÜ†Î¶¨

Ïù¥ Î¨∏ÏÑúÎäî ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§. Ïù¥Ï†Ñ Í≤∞Í≥ºÏôÄ ÎπÑÍµêÌïòÎ†§Î©¥ `reports/history/` ÎîîÎ†âÌÜ†Î¶¨Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.

---
*Generated by feedback-loop.py*
"""
        
        return doc
    
    def compare_with_baseline(self, current: Dict, baseline_scenario: str = "baseline") -> Dict:
        """Compare current results with baseline"""
        baseline_file = self.history_dir / "baseline_reference.json"
        
        if not baseline_file.exists():
            # Save current baseline as reference
            if baseline_scenario in current:
                with open(baseline_file, "w") as f:
                    json.dump(current[baseline_scenario], f, indent=2)
                return {"message": "Baseline reference created"}
        
        with open(baseline_file) as f:
            baseline = json.load(f)
        
        current_data = current.get(baseline_scenario, {}).get("summary", {})
        baseline_data = baseline.get("summary", {})
        
        comparison = {
            "p95_change": self._calc_change(
                current_data.get("p95_duration_ms", 0),
                baseline_data.get("p95_duration_ms", 1)
            ),
            "error_rate_change": self._calc_change(
                current_data.get("error_rate", 0),
                baseline_data.get("error_rate", 0.001)
            ),
            "throughput_change": self._calc_change(
                current_data.get("requests_per_second", 0),
                baseline_data.get("requests_per_second", 1)
            ),
        }
        
        return comparison
    
    def _calc_change(self, current: float, baseline: float) -> Dict:
        """Calculate percentage change"""
        if baseline == 0:
            return {"value": 0, "direction": "unchanged"}
        
        change = ((current - baseline) / baseline) * 100
        direction = "improved" if change < 0 else "degraded" if change > 0 else "unchanged"
        
        return {
            "value": abs(change),
            "direction": direction,
            "current": current,
            "baseline": baseline,
        }
    
    def run(self, output_dir: Optional[str] = None) -> Dict:
        """Run the complete feedback loop"""
        output_dir = Path(output_dir) if output_dir else self.latest_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Load results
        results = self.load_latest_results()
        
        if not results:
            return {"error": "No test results found"}
        
        # Analyze results
        analysis = self.analyze_results(results)
        
        # Generate action items
        action_items = self.generate_action_items(analysis)
        
        # Generate improvements document
        improvements_doc = self.generate_improvements_document(analysis, action_items)
        
        # Compare with baseline
        comparison = self.compare_with_baseline(results)
        
        # Save outputs
        with open(output_dir / "analysis.json", "w") as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        with open(output_dir / "action_items.json", "w") as f:
            json.dump(action_items, f, indent=2, ensure_ascii=False)
        
        with open(output_dir / "improvements.md", "w") as f:
            f.write(improvements_doc)
        
        with open(output_dir / "baseline_comparison.json", "w") as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Feedback loop completed")
        print(f"   - Analysis: {output_dir / 'analysis.json'}")
        print(f"   - Action Items: {output_dir / 'action_items.json'}")
        print(f"   - Improvements: {output_dir / 'improvements.md'}")
        
        return {
            "analysis": analysis,
            "action_items": action_items,
            "comparison": comparison,
        }


def main():
    parser = argparse.ArgumentParser(description="Run feedback loop on test results")
    parser.add_argument("--project-dir", default=".",
                       help="Project directory")
    parser.add_argument("--output-dir", 
                       help="Output directory (default: reports/latest)")
    parser.add_argument("--json", action="store_true",
                       help="Output results as JSON")
    
    args = parser.parse_args()
    
    # Resolve project directory
    script_dir = Path(__file__).parent
    project_dir = script_dir.parent if args.project_dir == "." else Path(args.project_dir)
    
    feedback = FeedbackLoop(str(project_dir))
    result = feedback.run(args.output_dir)
    
    if args.json:
        print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()

